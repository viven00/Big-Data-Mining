{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b823166",
   "metadata": {},
   "source": [
    "Classification Algorithms\n",
    "- Random Forest\n",
    "- Naive Bayes Classifier\n",
    "- Decisioin Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d114d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql.functions import abs\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65cf48ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/viventan/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/01 15:00:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b80bc",
   "metadata": {},
   "source": [
    "### Discover and visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8285f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Elevation: int, Aspect: int, Slope: int, horHydro: int, verHydro: int, horRoad: int, 9am: int, Noon: int, 3pm: int, horFire: int, wildAr_1: int, wildAr_2: int, wildAr_3: int, wildAr_4: int, soilType_1: int, soilType_2: int, soilType_3: int, soilType_4: int, soilType_5: int, soilType_6: int, soilType_7: int, soilType_8: int, soilType_9: int, soilType_10: int, soilType_11: int, soilType_12: int, soilType_13: int, soilType_14: int, soilType_15: int, soilType_16: int, soilType_17: int, soilType_18: int, soilType_19: int, soilType_20: int, soilType_21: int, soilType_22: int, soilType_23: int, soilType_24: int, soilType_25: int, soilType_26: int, soilType_27: int, soilType_28: int, soilType_29: int, soilType_30: int, soilType_31: int, soilType_32: int, soilType_33: int, soilType_3a4: int, soilType_35: int, soilType_36: int, soilType_37: int, soilType_38: int, soilType_39: int, soilType_40: int, cover: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from csv file\n",
    "df = spark.read.csv(\"/Users/viventan/Desktop/CSCI 316 Big Data Mining Techniques and Implementation/group ass 2/covtype.data\", inferSchema=True)\n",
    "\n",
    "# define the header for each columns\n",
    "df = df.toDF(\"Elevation\",\"Aspect\",\"Slope\",\"horHydro\",\"verHydro\",\"horRoad\",\"9am\",\\\n",
    "          \"Noon\",\"3pm\",\"horFire\",\"wildAr_1\",\"wildAr_2\",\"wildAr_3\",\"wildAr_4\",\\\n",
    "          \"soilType_1\",\"soilType_2\",\"soilType_3\",\"soilType_4\",\"soilType_5\",\"soilType_6\",\\\n",
    "          \"soilType_7\",\"soilType_8\",\"soilType_9\",\"soilType_10\",\"soilType_11\",\"soilType_12\",\\\n",
    "          \"soilType_13\",\"soilType_14\",\"soilType_15\",\"soilType_16\",\"soilType_17\",\"soilType_18\",\\\n",
    "          \"soilType_19\",\"soilType_20\",\"soilType_21\",\"soilType_22\",\"soilType_23\",\"soilType_24\",\\\n",
    "          \"soilType_25\",\"soilType_26\",\"soilType_27\",\"soilType_28\",\"soilType_29\",\"soilType_30\",\\\n",
    "          \"soilType_31\",\"soilType_32\",\"soilType_33\",\"soilType_3a4\",\"soilType_35\",\"soilType_36\",\\\n",
    "          \"soilType_37\",\"soilType_38\",\"soilType_39\",\"soilType_40\",\"cover\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7ab1b",
   "metadata": {},
   "source": [
    "# Checking PySpark DataFrame schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the skewness of each features\n",
    "print('Skewness of all the features: ')\n",
    "skewness = df2.skew()\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure size \n",
    "plt.figure(figsize=(16,18))\n",
    "\n",
    "# plot out the diagram to have a clear view of the skewness\n",
    "sns.barplot(x=df2.skew(), y=skewness.index, palette = 'gist_rainbow_r')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce78724",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Many Soil_Type have the extreme right skewed, especially the Soil_Type_15.\n",
    "\n",
    "(2) The feature 'Hillshade_9am' and 'Hillshade_Noon' have the extreme left skewed which score -1.18 and -1.06 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  returns the correlation matrix of the dataframe\n",
    "df2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7122278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correlation of all columns in this dataframe\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# using heapmap to plot\n",
    "sns.heatmap(df2.corr(), annot=True)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714e7d8",
   "metadata": {},
   "source": [
    "That looks abit cranked up for a correlation chart. Hence, we will reduce the number of attributes to just soil_types against the cover_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41789d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and store all the Soil Type features\n",
    "columns = [cols for cols in df2.columns if str(cols).startswith('Soil')]\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# using heapmap to plot\n",
    "sns.heatmap(df2[columns].corr(), square=True, linewidths=1)\n",
    "\n",
    "# set the title of the plot\n",
    "plt.title('Correlation of all Soil Type features')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4002ffc",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) There is not any correlation between the soil_types. Hence, we try the remaining columns to see their correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and store all the not Soil Type features\n",
    "columns = [cols for cols in df2.columns if not str(cols).startswith('Soil')]\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# using heapmap to plot\n",
    "sns.heatmap(df2[columns].corr(), square=True, linewidths=1, annot = True)\n",
    "\n",
    "# set the title of the plot\n",
    "plt.title('Correlation of the features')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a5ff3",
   "metadata": {},
   "source": [
    "#### Findings: \n",
    "\n",
    "Based on above, we can notice that:\n",
    "\n",
    "(1) Wilderness_Area_4 has a strong positive correlation with the target (Cover_Type) which gives a 0.32. Hence, it means that from Wilderness_Area_4, we can get a good information for the Cover Type.\n",
    "\n",
    "(2) Next, Slope is second one that strong correlated with Cover Tpe which gives 0.15.\n",
    "\n",
    "(3) The most weak correlation is Elevation which has a negative correlation with the target (Cover_Type) and it gives -0.27. Hence, it means that from Elevation, we cannot get any good information about the Cover Type.\n",
    "\n",
    "(4) The second weak correlation is Wilderness_Area_1 which give a negative correlation -0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the tota number of each cover types in this dataframe\n",
    "df2.groupby('Cover_Type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854262cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the count of each 'Cover_Type' in this dataframe\n",
    "plt.title('Count of each Cover_Type in this dataframe')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.countplot(x=df2['Cover_Type'], palette = 'gist_rainbow_r')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019474a4",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) The most number of count of Cover Type is Cover_Type 2. Then, second more count of Cover Type is Cover_Type 1. Both informations show that Cover_Type 1 and 2 are the most common cover type in the wildness area.\n",
    "\n",
    "(2) The least number of count of Cover Type is Cover_Type 4 which is less appear in the wildness area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4004783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and elevation\n",
    "plt.title('The relationship between Cover_Type and Elevation')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Elevation', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e80e3b",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Cover_Type 7 is in the highest elevation level which median falls around 3315. Also, it contains lower outlier falls between 2750 and 3125 and upper outlier which falls between 3500 and 3858.\n",
    "\n",
    "(2) Comapare with other Cover Type, we notice that most of the Cover_Type 4 are in the lowest elevation level which median falls around 2250 and it does not contain any lower and upper outliers. Its minimum elevation is 2000 and max elevation is around 2600.\n",
    "\n",
    "(3) Cover Type 3 and Cover Type 4 has no any outliers.\n",
    "\n",
    "(4) Since all the boxes are very different from each others. Hence, the elevation is a good discriminative feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and aspect\n",
    "plt.title('The relationship between Cover_Type and Aspect')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Aspect', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802bf1b",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Most of the Cover Type do not have any lower and upper outliers, except Cover Type 4 contains upper outlier which falls between 275 and 360.\n",
    "\n",
    "(2) The median of Cover Type 1 and Cover Type 7 is the same at 125.\n",
    "\n",
    "(3) The minimum and maximum aspect of all Cover Types are the same which is 0 and 360.\n",
    "\n",
    "(4) Most of the Cover Type's Aspect are in the range between 60 to 240.\n",
    "\n",
    "(5) This is a good feature since there has least number of outliers which can lead to a more accurate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and slope\n",
    "plt.title('The relationship between Cover_Type and Slope')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Slope', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb39ed",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Most of the Cover Types have upper outliers, except Cover Type 4 do not have any outliers in its upper.\n",
    "\n",
    "(2) Cover Type 2 has the most outliers which compare with other Cover Types and its outlier reaches to 66. It tells us that many unusual observation of Cover Type 2 is more than the maximum usual obseravation of Cover Type 2.\n",
    "\n",
    "(3) All of the Cover Types have the same minimum slope which is 0.\n",
    "\n",
    "(4) Cover Type 2 and Cover Type 7 have the same median which slope around 14 which tell us that 50% of their Cover Type are less than or equal to 14.\n",
    "\n",
    "(5) 25% of the slope of Cover Type 1 and Cover Type 2 are less than or equal to 8.\n",
    "\n",
    "(6) Cover Type 3 and Cover Type 4 have the same maximun about 46 in usual obeservation data (Cover Type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and Horizontal_Distance_To_Hydrology\n",
    "plt.title('The relationship between Cover_Type and Horizontal_Distance_To_Hydrology')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Horizontal_Distance_To_Hydrology', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea3de4",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Most of the Cover Type of their interquartile range (25% ~ 75%) are between 0 to 400, except Cover Type 7.\n",
    "\n",
    "(2) All the Cover Types have upper outliers, especially Cover Type 1, 2 and 5 where Cover Type 2 reaches 1397.\n",
    "\n",
    "(3) For Cover Type 4, most of them reach 0, and its maximun value is lower than others as well as it contains the least of upper outliers.\n",
    "\n",
    "(4) Cover Type 7 has the largest maximun value of the normal observation data which compare to the rest and it reach around 1100.\n",
    "\n",
    "(5) Most of the Cover Type's maximun value are approximately between 500 and 650."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and Vertical_Distance_To_Hydrology\n",
    "plt.title('The relationship between Cover_Type and Vertical_Distance_To_Hydrology')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Distance_To_Hydrology', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f72a7",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) 50% of most of the Cover Type of Vertical_Distance_To_Hydrology is less than or equal to 10.\n",
    "\n",
    "(2) Cover Type 1 and Cover Type 2 contains lots of upper outliers and the largest outlier reaches 601.\n",
    "\n",
    "(3) Cover Type 2 also has most lower outliers which reaches -173.\n",
    "\n",
    "(4) Over the all Cover Types, we can see that the minimum Vertical_Distance_To_Hydrology of Cover Type 4 is higher than all of the Cover Type which reaches around -18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f46f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and Horizontal_Distance_To_Roadways\n",
    "plt.title('The relationship between Cover_Type and Horizontal_Distance_To_Roadways')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Distance_To_Roadways', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659837a9",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Cover Type 5 has a very far outliers from its normal observation data which could affect the machine learning model to do the prediction.\n",
    "\n",
    "(2) Most of the cover types can be found nearby the roadway in the view of horizontal distance, except Cover_Type 7.\n",
    "\n",
    "(3) Cover_Type 4 have no any outliers and its maximun value of Horizontal_Distance_To_Roadways is also the lowest one. Based on this information, we can say that most of the Cover Type 4 are nearby the roaadway in the view of Horizontal_Distance_To_Roadways which compare with others.\n",
    "\n",
    "(4) Other than Cover Type 4, Cover Type 1 and 7 also have no any lower and upper outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and Hillshade_9am\n",
    "plt.title('The relationship between Cover_Type and Hillshade_9am')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Hillshade_9am', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7cc09",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) As above boxplots, all the Cover Types only have lower outliers, and most of the Cover Types have the extreme outliers far from its maximun point.\n",
    "\n",
    "(2) The Hillshade_9am has a left skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d73d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and Hillshade_Noon\n",
    "plt.title('The relationship between Cover_Type and Hillshade_Noon')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Hillshade_Noon', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6eda4f",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) As above boxplots, all the Cover Types only have lower outliers, and Cover Type 2 has a extreme outlier far from its maximun value.\n",
    "\n",
    "(2) The Hillshade_Noon has a left skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and Hillshade_3pm\n",
    "plt.title('The relationship between Cover_Type and Hillshade_3pm')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Hillshade_3pm', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b532605",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Only Cover Type 3, 4 and 5 do not have any lower and upper outliers. Also, their minimun and maximun value are reach to 0 and near 250 respectively.\n",
    "\n",
    "(2) Hillshade_3pm has a normal skew.\n",
    "\n",
    "(3) Cover Type 1, 2, 3, 7 have the same median.\n",
    "\n",
    "(4) Cover Type 1 and 2 have the upper outliers which over 250.\n",
    "\n",
    "(5) Cover Type 4 and 5's median are lower than the average median of other Cover Types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6184121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between cover_type and Horizontal_Distance_To_Fire_Points\n",
    "plt.title('The relationship between Cover_Type and Horizontal_Distance_To_Fire_Points')\n",
    "sns.boxplot(data=df2, x='Cover_Type', y='Horizontal_Distance_To_Fire_Points', palette='gist_rainbow_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34220d07",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) As above, we can see that Cover Type 1 and 2 are mostly the same box shape and same median around 1900 as well as its upper outliers are also the same which almost reach 7173. Next, the maximum value of Horizontal_Distance_To_Fire_Points for both are also near 5000. Besides that, their interquartile range (25% ~ 75%) are almost the same which between 1000 and 2900.\n",
    "\n",
    "(2) Only Cover Type 4 and 7 do not contain any upper and lower outliers.\n",
    "\n",
    "(3) For Cover Type 5, it has some upper outliers are very far from its maximun value which would cause the model not that accuracy when doing prediction.\n",
    "\n",
    "(4) For Cover Type 3 and 4, they mostly have the same median around 800 and their interquartile range (25% ~ 75%) are almost the same which between 500 and 1200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd19cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the columns that want to plot out\n",
    "areas = ['Wilderness_Area_1', 'Wilderness_Area_2', 'Wilderness_Area_3', 'Wilderness_Area_4']\n",
    "    \n",
    "# using for loop to generate the graph\n",
    "for i in range(len(areas)):\n",
    "    \n",
    "    # plot i number of graphs\n",
    "    plt.figure(i, figsize=(12, 7))\n",
    " \n",
    "    # group by each of the cover type and sum each of them\n",
    "    df2.groupby('Cover_Type')[areas[i]].sum().plot.bar(width = 0.4, color='#ffbf00')\n",
    "    \n",
    "    # display the title name\n",
    "    plt.title(areas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391b501",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Most of the Cover Type 1 and 2 are grown in Wilderness_Area_1, Wilderness_Area_2 and Wilderness_Area_3. Only have few of the Cover Type 2 grow in Wilderness_Area_4.\n",
    "\n",
    "(2) For Cover Type 3, it mainly distributes in Wilderness_Area_4 and got few distribute in Wilderness_Area_3.\n",
    "\n",
    "(3) Very interesting finding is that for Cover Type 4 which only distribute in Wilderness_Area_4. Other Wilderness_Area do not exist any Cover Type 4 and the amount of Cover Type also less.\n",
    "\n",
    "(4) For Cover Type 5, it only could be found in Wilderness_Area_1 and Wilderness_Area_3. The amount of Cover Type also less.\n",
    "\n",
    "(5) Cover Type 6 mainly found in Wilderness_Area_4 and few could be found in Wilderness_Area_3\n",
    "\n",
    "(7) For Cover Type 7, it could not be found only in Wilderness_Area_4 and the rest have the existence of Cover Type 7.\n",
    "\n",
    "(8) In Wilderness_Area_3, we could find most of the Cover Types, except the Cover Type 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the columns name\n",
    "all_columns = df.columns\n",
    "\n",
    "# eliminate th target column\n",
    "eliminate_target = len(all_columns)-1\n",
    "\n",
    "# x-axis is for target column\n",
    "x = all_columns[eliminate_target]\n",
    "\n",
    "# y-axis is for the features\n",
    "y = all_columns[14:eliminate_target]\n",
    "\n",
    "# plot out the graph\n",
    "for a in range(0, 40):\n",
    "    sns.violinplot(data=df2, x=x, y=y[a], palette=\"muted\")\n",
    "    \n",
    "    # set the title for each of the graphes\n",
    "    plt.title('Relationship between Cover_Type and Soil_Type_' + str(a+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc96e6f",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Based on the 40 violinplot graphes above, we can conclude that for Soil_Type 1, 5, 7, 8, 9, 12, 14, 15 18, 21, 22, 25-28 and 35-40 which have at least 4 Cover Types do not have any values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cefa5",
   "metadata": {},
   "source": [
    "### Prepare the data for machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c4c61",
   "metadata": {},
   "source": [
    "### We first check if the dataset consist of any missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5733928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check any missing data for each columns\n",
    "print(df2.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4cca5",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) It gives a False rusult which means that this dataframe does not consist any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1483392a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: DataFrame[Elevation: int, Aspect: int, Slope: int, horHydro: int, verHydro: int, horRoad: int, 9am: int, Noon: int, 3pm: int, horFire: int, wildAr_1: int, wildAr_2: int, wildAr_3: int, wildAr_4: int, soilType_1: int, soilType_2: int, soilType_3: int, soilType_4: int, soilType_5: int, soilType_6: int, soilType_7: int, soilType_8: int, soilType_9: int, soilType_10: int, soilType_11: int, soilType_12: int, soilType_13: int, soilType_14: int, soilType_15: int, soilType_16: int, soilType_17: int, soilType_18: int, soilType_19: int, soilType_20: int, soilType_21: int, soilType_22: int, soilType_23: int, soilType_24: int, soilType_25: int, soilType_26: int, soilType_27: int, soilType_28: int, soilType_29: int, soilType_30: int, soilType_31: int, soilType_32: int, soilType_33: int, soilType_3a4: int, soilType_35: int, soilType_36: int, soilType_37: int, soilType_38: int, soilType_39: int, soilType_40: int, cover: int] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dx/5k_5m77129d316n8tl0pzf4m0000gn/T/ipykernel_735/1460706473.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;34m\"Soil_Type_31\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Soil_Type_32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Soil_Type_33\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Soil_Type_34\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Soil_Type_35\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;34m\"Soil_Type_36\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Soil_Type_37\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Soil_Type_38\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Soil_Type_39\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \"Soil_Type_40\", \"Cover_Type\", abs(df))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mabs\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mComputes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mabsolute\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function_over_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"abs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_invoke_function_over_column\u001b[0;34m(name, col)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mwraps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34m\"Invalid argument, not a string or column: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: DataFrame[Elevation: int, Aspect: int, Slope: int, horHydro: int, verHydro: int, horRoad: int, 9am: int, Noon: int, 3pm: int, horFire: int, wildAr_1: int, wildAr_2: int, wildAr_3: int, wildAr_4: int, soilType_1: int, soilType_2: int, soilType_3: int, soilType_4: int, soilType_5: int, soilType_6: int, soilType_7: int, soilType_8: int, soilType_9: int, soilType_10: int, soilType_11: int, soilType_12: int, soilType_13: int, soilType_14: int, soilType_15: int, soilType_16: int, soilType_17: int, soilType_18: int, soilType_19: int, soilType_20: int, soilType_21: int, soilType_22: int, soilType_23: int, soilType_24: int, soilType_25: int, soilType_26: int, soilType_27: int, soilType_28: int, soilType_29: int, soilType_30: int, soilType_31: int, soilType_32: int, soilType_33: int, soilType_3a4: int, soilType_35: int, soilType_36: int, soilType_37: int, soilType_38: int, soilType_39: int, soilType_40: int, cover: int] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "# Apply Abs to columns with negative values\n",
    "df3 = df.withColumn(\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n",
    "                \"Distance_To_Hydrology\", \"Distance_To_Roadways\",\n",
    "                \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "                \"Horizontal_Distance_To_Fire_Points\", \"Wilderness_Area_1\", \"Wilderness_Area_2\",\n",
    "                \"Wilderness_Area_3\", \"Wilderness_Area_4\", \"Soil_Type_1\", \n",
    "                \"Soil_Type_2\", \"Soil_Type_3\", \"Soil_Type_4\", \"Soil_Type_5\", \n",
    "                \"Soil_Type_6\", \"Soil_Type_7\", \"Soil_Type_8\", \"Soil_Type_9\", \"Soil_Type_10\", \n",
    "                \"Soil_Type_11\", \"Soil_Type_12\", \"Soil_Type_13\", \"Soil_Type_14\", \"Soil_Type_15\", \n",
    "                \"Soil_Type_16\", \"Soil_Type_17\", \"Soil_Type_18\", \"Soil_Type_19\", \"Soil_Type_20\", \n",
    "                \"Soil_Type_21\", \"Soil_Type_22\", \"Soil_Type_23\", \"Soil_Type_24\", \"Soil_Type_25\", \n",
    "                \"Soil_Type_26\", \"Soil_Type_27\", \"Soil_Type_28\", \"Soil_Type_29\", \"Soil_Type_30\", \n",
    "                \"Soil_Type_31\", \"Soil_Type_32\", \"Soil_Type_33\", \"Soil_Type_34\", \"Soil_Type_35\", \n",
    "                \"Soil_Type_36\", \"Soil_Type_37\", \"Soil_Type_38\", \"Soil_Type_39\", \n",
    "                \"Soil_Type_40\", \"Cover_Type\", abs(df.Elevation,Aspect,Slope,horHydro,verHydro,horRoad,9am,\\\n",
    "          Noon,3pm,horFire,wildAr_1,wildAr_2,wildAr_3,wildAr_4,\\\n",
    "          soilType_1,soilType_2,soilType_3,soilType_4,soilType_5,soilType_6,\\\n",
    "          soilType_7,soilType_8,soilType_9,soilType_11,soilType_12\",\\\n",
    "          soilType_13,soilType_14,soilType_15,soilType_16,soilType_17,soilType_18\",\\\n",
    "          soilType_19,soilType_20,soilType_21,soilType_22,soilType_23,soilType_24\",\\\n",
    "          soilType_25,soilType_26,soilType_27,soilType_28,soilType_29,soilType_30\",\\\n",
    "          soilType_31,soilType_32,soilType_33,soilType_3a4,soilType_35,soilType_36\",\\\n",
    "          soilType_37,soilType_38,soilType_39,soilType_40\"))\n",
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017eeb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with negative values\n",
    "df = df4.drop(\"Distance_To_Hydrology\",\"Distance_To_Roadways\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c0b6a",
   "metadata": {},
   "source": [
    "## Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dfe950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vector_assembler = VectorAssembler(\\\n",
    "inputCols=[\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n",
    "                \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "                \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "                \"Horizontal_Distance_To_Fire_Points\", \"Wilderness_Area_1\", \"Wilderness_Area_2\",\n",
    "                \"Wilderness_Area_3\", \"Wilderness_Area_4\", \"Soil_Type_1\", \n",
    "                \"Soil_Type_2\", \"Soil_Type_3\", \"Soil_Type_4\", \"Soil_Type_5\", \n",
    "                \"Soil_Type_6\", \"Soil_Type_7\", \"Soil_Type_8\", \"Soil_Type_9\", \"Soil_Type_10\", \n",
    "                \"Soil_Type_11\", \"Soil_Type_12\", \"Soil_Type_13\", \"Soil_Type_14\", \"Soil_Type_15\", \n",
    "                \"Soil_Type_16\", \"Soil_Type_17\", \"Soil_Type_18\", \"Soil_Type_19\", \"Soil_Type_20\", \n",
    "                \"Soil_Type_21\", \"Soil_Type_22\", \"Soil_Type_23\", \"Soil_Type_24\", \"Soil_Type_25\", \n",
    "                \"Soil_Type_26\", \"Soil_Type_27\", \"Soil_Type_28\", \"Soil_Type_29\", \"Soil_Type_30\", \n",
    "                \"Soil_Type_31\", \"Soil_Type_32\", \"Soil_Type_33\", \"Soil_Type_34\", \"Soil_Type_35\", \n",
    "                \"Soil_Type_36\", \"Soil_Type_37\", \"Soil_Type_38\", \"Soil_Type_39\", \n",
    "                \"Soil_Type_40\"],\\\n",
    "outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_indexer = StringIndexer(inputCol=\"Cover_Type\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815a23c",
   "metadata": {},
   "source": [
    "## Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966aa7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 2/3 cover-type into training set\n",
    "train = df.sampleBy(\"Cover_Type\", fractions={1:2/3,2:2/3,3:2/3,4:2/3,5:2/3,6:2/3,7:2/3}, seed=0)\n",
    "# Subtracting 'train' from original 'data' to get test set \n",
    "test = df.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b10f96e",
   "metadata": {},
   "source": [
    "# 3 Clssification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c5d2f",
   "metadata": {},
   "source": [
    "## (1) Decision Tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf6ce9",
   "metadata": {},
   "source": [
    "- Strength \n",
    "   - i.  Easy to interpret\n",
    "   - ii. Handle categorical features\n",
    "   - iii. Able to capture non-linearities and feature interactions\n",
    "\n",
    "- Weaknesses\n",
    "   - i. A small change in the data can cause a large change in the structure of the decision tree causing instability\n",
    "\n",
    "   - ii. Calculations can be more complex compared to other algorithms\n",
    "   - iii. longer training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a3d96",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf338c",
   "metadata": {},
   "source": [
    "- Pipeline is to combine multiple steps into a single workflow that can be easily run.\n",
    "- Build a pipeline combining all of our feature engineering and modeling steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e24394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    " \n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=[l_indexer, vector_assembler, dt])\n",
    " \n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(train)\n",
    " \n",
    "# Apply the pipeline model to the test dataset.\n",
    "predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0429e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predictions from the model\n",
    "predictions.select(\"prediction\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157b464",
   "metadata": {},
   "source": [
    "### Area under ROC curve\n",
    "- As the area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute the area under the ROC curve \n",
    "BCevaluator = BinaryClassificationEvaluator()\\\n",
    "              .setMetricName(\"areaUnderROC\")\\\n",
    "              .setRawPredictionCol(\"prediction\")\\\n",
    "              .setLabelCol(\"label\")\n",
    "print(f\"Area under ROC curve : {BCevaluator.evaluate(predictions)}\")\n",
    "\n",
    "# evalute the accuracy\n",
    "MCevaluator = MulticlassClassificationEvaluator(\\\n",
    "              labelCol=\"label\", predictionCol=\"prediction\",\\\n",
    "              metricName=\"accuracy\")\n",
    "print(f\"Accuracy: {MCevaluator.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce595b1",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    " (1) The area under ROC curve is 77.01% and it is considered acceptable.\n",
    " \n",
    " (2) Accuracy before tuning is 70.22%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5801a",
   "metadata": {},
   "source": [
    "## Tuning the Decisioin Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a grid search over a set of model hyperparameters.\n",
    "params = ParamGridBuilder().addGrid(dt.maxDepth, [10,12,14]).addGrid(dt.maxBins, [7,8]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352711f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainValidationSplit\n",
    "tvs = TrainValidationSplit().setTrainRatio(2/3).setEstimatorParamMaps(params).setEstimator(pipeline).setEvaluator(BCevaluator)\n",
    "\n",
    "cvModel = tvs.fit(train)\n",
    "# Use the model identified by the TrainValidationSplit to make predictions on the test dataset\n",
    "cvPredDF = cvModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1893673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance based on area under the ROC curve and accuracy \n",
    "print(f\"Area under ROC curve: {BCevaluator.evaluate(cvPredDF)}\")\n",
    "print(f\"Accuracy: {MCevaluator.evaluate(cvPredDF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2bddbc",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "After tuning the model:\n",
    "(1) First, we can see that the area under ROC curve becomes 85.40% which compare with the area under ROC curve 77.01% (without tuning) has an significant improvement of 8.39% and this is considered excellent.\n",
    "\n",
    "(2) Next, we can see that the accuracy becomes 82.15% which compare with the accuracy 70.22% (without tuning) has an significant improvement of 11.93%.\n",
    "\n",
    "(3)  Furthermore, We selected 3 options for the maxdepth parameter and 2 options for maxBIns parameters then using the TrainValidationSplit to make the predictions and it gives us a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c3855",
   "metadata": {},
   "source": [
    "## (2) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293afbc",
   "metadata": {},
   "source": [
    "- Strength \n",
    "   - i.  Works well with both categorical and continuous values\n",
    "   - ii. Comparatively less impacted by noise\n",
    "   - iii. Automatically handle missing values\n",
    "\n",
    "- Weaknesses\n",
    "   - i. Requires much computational power and resources as it builds numerous trees to combine their outputs. \n",
    "\n",
    "   - ii. Requires much more time to train as compared to decision trees as it generates a lot of trees and makes decision on the majority of votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75db750",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\",\\\n",
    "featuresCol=\"features\", numTrees=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6c785",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    " \n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=[l_indexer, vector_assembler, rf])\n",
    " \n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(train)\n",
    " \n",
    "# Apply the pipeline model to the test dataset.\n",
    "predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20348af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predictions from the model\n",
    "predictions.select(\"prediction\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute the area under the ROC curve \n",
    "BCevaluator = BinaryClassificationEvaluator()\\\n",
    "              .setMetricName(\"areaUnderROC\")\\\n",
    "              .setRawPredictionCol(\"prediction\")\\\n",
    "              .setLabelCol(\"label\")\n",
    "print(f\"Area under ROC curve : {BCevaluator.evaluate(predictions)}\")\n",
    "\n",
    "# evalute the accuracy\n",
    "MCevaluator = MulticlassClassificationEvaluator(\\\n",
    "              labelCol=\"label\", predictionCol=\"prediction\",\\\n",
    "              metricName=\"accuracy\")\n",
    "print(f\"Accuracy: {MCevaluator.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c00a8",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    " (1) The area under ROC curve is 74.26% and it is considered acceptable.\n",
    " \n",
    " (2) Accuracy before tuning is 67.30%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821eb927",
   "metadata": {},
   "source": [
    "### Tuning the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9af5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a grid search over a set of model hyperparameters.\n",
    "params = (ParamGridBuilder()\n",
    "             #.addGrid(rf.maxDepth, [2, 5, 10, 20, 30])\n",
    "               .addGrid(rf.maxDepth, [2, 5, 10])\n",
    "             #.addGrid(rf.maxBins, [10, 20, 40, 80, 100])\n",
    "               .addGrid(rf.maxBins, [5, 10, 20])\n",
    "             #.addGrid(rf.numTrees, [5, 20, 50, 100, 500])\n",
    "               .addGrid(rf.numTrees, [5, 20, 50])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69230ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainValidationSplit\n",
    "tvs = TrainValidationSplit().setTrainRatio(2/3).setEstimatorParamMaps(params).setEstimator(pipeline).setEvaluator(BCevaluator)\n",
    "\n",
    "cvModel = tvs.fit(train)\n",
    "# Use the model identified by the TrainValidationSplit to make predictions on the test dataset\n",
    "cvPredDF = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb915f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance based on area under the ROC curve and accuracy \n",
    "print(f\"Area under ROC curve: {BCevaluator.evaluate(cvPredDF)}\")\n",
    "print(f\"Accuracy: {MCevaluator.evaluate(cvPredDF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d23d05",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "After tuning the model:\n",
    "\n",
    "(1) First, we can see that the area under ROC curve becomes 78.86% which compare with the area under ROC \n",
    "curve 74.26% (without tuning) has an improvement of 4.6% and this is considered quite good.\n",
    "\n",
    "(2) Next, we can see that the accuracy becomes 73.17% which compare with the accuracy 67.30% (without tuning) \n",
    "has an improvement of 5.87%.\n",
    "\n",
    "(3)  Furthermore, We selected 3 options for the maxdepth parameter,3 options for maxBIns parameters and \n",
    "3 options for numTrees then using the TrainValidationSplit to make the predictions and it gives us a better \n",
    "accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204cd60",
   "metadata": {},
   "source": [
    "## (3) Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974d374",
   "metadata": {},
   "source": [
    "- Strengths:\n",
    "    - i. Able to handle an extremely large number of features\n",
    "    - ii. Relatively simple to implement and tune the model.\n",
    "    \n",
    "- Weaknessess:\n",
    "    - i. Does not solve negative values in the dataset. Hence, we apply abs() function to columns containing negative values in the dataset.\n",
    "    - ii. If categorical variable has a category in test data set, which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the nb and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e4a1a",
   "metadata": {},
   "source": [
    "### Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=[l_indexer, vector_assembler, nb])\n",
    " \n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(train)\n",
    " \n",
    "# Apply the pipeline model to the test dataset.\n",
    "predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9140e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BCevaluator = BinaryClassificationEvaluator()\\\n",
    "          .setMetricName(\"areaUnderROC\")\\\n",
    "          .setRawPredictionCol(\"prediction\")\\\n",
    "          .setLabelCol(\"label\")\n",
    "print(f\"Area under ROC curve : {BCevaluator.evaluate(predictions)}\")\n",
    "\n",
    "MCevaluator = MulticlassClassificationEvaluator(\\\n",
    "labelCol=\"label\", predictionCol=\"prediction\",\\\n",
    "metricName=\"accuracy\")\n",
    "print(f\"Accuracy: {MCevaluator.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4030aa",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    " (1) The area under ROC curve is 53.01% and it is considered acceptable.\n",
    " \n",
    " (2) Accuracy before tuning is 27.14%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829ca54",
   "metadata": {},
   "source": [
    "### Tuning Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamGridBuilder().addGrid(nb.smoothing, [0.5, 0.7, 0.9, 1.1, 1.2, 1.0]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainValidationSplit\n",
    "tvs = TrainValidationSplit().setTrainRatio(2/3).setEstimatorParamMaps(params).setEstimator(pipeline).setEvaluator(MCevaluator)\n",
    "\n",
    "cvModel = tvs.fit(train)\n",
    "# Use the model identified by the TrainValidationSplit to make predictions on the test dataset\n",
    "cvPredDF = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance based on area under the ROC curve and accuracy \n",
    "print(f\"Area under ROC curve: {BCevaluator.evaluate(cvPredDF)}\")\n",
    "print(f\"Accuracy: {MCevaluator.evaluate(cvPredDF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307902b",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "After tuning the model:\n",
    "(1) First, we can see that the area under ROC curve becomes 53.00% which compare with the area under ROC curve 53.01% (without tuning) has only an improvement of 0.01%.\n",
    "\n",
    "(2) Next, we can see that the accuracy becomes 27.16% which compare with the accuracy 27.14% (without tuning) has an improvement of 0.02%.\n",
    "\n",
    "(3) Furthermore, We selected 6 options for var smoothing then using the TrainValidationSplit to make the predictions and the accuracy does not improved much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3eeb7",
   "metadata": {},
   "source": [
    "#### Finding of comparing these 3 classification models above:\n",
    "    \n",
    "In this task, we have used Decisioin Tree Classifier,Random Forest model and Naive Bayes Classifier model to solve the classification problem.\n",
    "\n",
    "Among these 3 models, Random Forest model and Decisioin Tree Classifier model work quite well in this task. Decisioin Tree Classifier gets an approximately 70% and 82% before and after tuning the models. Also, Random Forest model gets an approiximately 67% and 73% before and after tuning the model.\n",
    "\n",
    "However, Naive Bayes Classifier model gives a poor result in solving the classification problem of this task even after tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e9a01",
   "metadata": {},
   "source": [
    "## User-Defined Transform functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizer(Elevation):\n",
    "  if Elevation < 2500:\n",
    "    return 1\n",
    "  elif Elevation < 3000:\n",
    "    return 2\n",
    "  else :\n",
    "    return 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fc358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf,col\n",
    "  \n",
    "elevation_level = udf(lambda Elevation: categorizer(Elevation), IntegerType())\n",
    "df_new = df.withColumn('Elevation_lvl', elevation_level(col('Elevation')))\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ab3f4",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) We generate a new feature called 'Elevation_lvl' since the feature 'Elevation' contains many different values and do not have a specific range for the models to do prediction.\n",
    "\n",
    "(2) The new feature 'Elevation_lvl' has categorize the Elevation values into 3 catrgories which has 1, 2 and 3 so that we can based on this new feature to see the elevation each Cover Type belong to low, median or high. But in our case, we use number instead of string to label the category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.groupBy(\"Elevation_lvl\",\"Cover_Type\").count().orderBy(\"Elevation_lvl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5ee89",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "(1) Based on the counts above, we can see that most of the Cover Types are in Elevation_Lvl 1 and 2, except Cover Type 7 which does not appear any in Elevation_Level 1. Also, the elevation level of Cover Type 1,2 and 5 are average distribution which appear in all 3 elevation level 1, 2 and 3. \n",
    "\n",
    "(2) Next, we notice that only Cover Type 1,2,5 and 7 have in Elevatioin_Level 3. Hence, we can conclude that Cover Type 3, 4, and 6 do not grow in the elevation level 3 based on this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
